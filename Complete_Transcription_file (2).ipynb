{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Set up"
      ],
      "metadata": {
        "id": "vMM8pHYXpymX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda : \"UTF-8\"\n",
        "!pip install -q openai-whisper ffmpeg"
      ],
      "metadata": {
        "id": "7KpdM3BdmJ1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code, import libraries, choose whisper model, record\n"
      ],
      "metadata": {
        "id": "sZTNwVuVpsm6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6OTcYWcmDrB"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "from io import BytesIO\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import detect_silence\n",
        "import whisper  # Open-source Whisper library\n",
        "\n",
        "JAVASCRIPT_RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=> {\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "# Function for recording with speech-to-text transcription\n",
        "def record_and_transcribe():\n",
        "    print(\"Start recording...\")\n",
        "\n",
        "    # Parameters for silence detection\n",
        "    silence_thresh = -40  # Silence threshold in dB\n",
        "    silence_duration = 1500  # Silence duration in milliseconds\n",
        "\n",
        "    audio_data = AudioSegment.silent(duration=0)  # Start with empty audio\n",
        "    transcription_file = \"/content/transcription.txt\"  # File to save transcription\n",
        "\n",
        "    # Clear the transcription file at the start\n",
        "    with open(transcription_file, \"w\") as f:\n",
        "        f.write(\"Transcription:\\n\")\n",
        "\n",
        "    # Load the Whisper model\n",
        "    model = whisper.load_model(\"base\")  # model size: tiny, base, small, medium, large\n",
        "\n",
        "    while True:\n",
        "        display(Javascript(JAVASCRIPT_RECORD))\n",
        "        js_audio = output.eval_js('record(3000)')  # Record 3-second chunks\n",
        "        b = b64decode(js_audio.split(',')[1])\n",
        "        chunk = AudioSegment.from_file(BytesIO(b))\n",
        "        audio_data += chunk  # Append the new chunk\n",
        "\n",
        "        # Save the chunk to a temporary file for transcription\n",
        "        chunk.export(\"/content/temp_chunk.wav\", format=\"wav\")\n",
        "\n",
        "        # Transcribe the chunk\n",
        "        transcription = transcribe_audio(\"/content/temp_chunk.wav\", model)\n",
        "        print(f\"Chunk Transcription: {transcription}\")\n",
        "\n",
        "        # Save the transcription to the file\n",
        "        with open(transcription_file, \"a\") as f:\n",
        "            f.write(transcription + \"\\n\")\n",
        "\n",
        "        # Detect silence in the audio\n",
        "        silence_ranges = detect_silence(audio_data, min_silence_len=silence_duration, silence_thresh=silence_thresh)\n",
        "\n",
        "        # Stop recording if the last chunk is silent\n",
        "        if silence_ranges and silence_ranges[-1][1] >= len(audio_data):\n",
        "            print(\"Silence detected. Stopping recording.\")\n",
        "            break\n",
        "\n",
        "    # Save the entire recorded audio\n",
        "    audio_data.export(\"/content/continuous_audio.wav\", format=\"wav\")\n",
        "    print(\"Recording saved as 'continuous_audio.wav'.\")\n",
        "    print(f\"Transcription saved as '{transcription_file}'.\")\n",
        "\n",
        "# Function for transcribing audio using local Whisper\n",
        "def transcribe_audio(file_path, model):\n",
        "    try:\n",
        "        result = model.transcribe(file_path)\n",
        "        return result[\"text\"]\n",
        "    except Exception as e:\n",
        "        return f\"Error during transcription: {str(e)}\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start recording"
      ],
      "metadata": {
        "id": "8ZfJk8gupqzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_and_transcribe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "SUmX1asCmYLZ",
        "outputId": "60fec89b-61d9-4e18-b8d9-2e902de8cdc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start recording...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=> {\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk Transcription:  For example, in two weeks\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=> {\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk Transcription:  and like\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=> {\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk Transcription:  if I can have a good grade on that please.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=> {\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk Transcription: \n",
            "Silence detected. Stopping recording.\n",
            "Recording saved as 'continuous_audio.wav'.\n",
            "Transcription saved as '/content/transcription.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out Transcription File"
      ],
      "metadata": {
        "id": "9i8-HsMipnxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File path\n",
        "transcription_file = \"/content/transcription.txt\"\n",
        "\n",
        "# Read and print the contents of the file\n",
        "try:\n",
        "    with open(transcription_file, \"r\") as f:\n",
        "        transcription_text = f.read()\n",
        "        print(\"Contents of transcription file:\\n\")\n",
        "        print(transcription_text)\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file '{transcription_file}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmOG1ZiLpNIA",
        "outputId": "f3648b8b-4e9a-4c61-9aaa-2944c2f1a6b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of transcription file:\n",
            "\n",
            "Transcription:\n",
            " For example, in two weeks\n",
            " and like\n",
            " if I can have a good grade on that please.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}